\section{Performance Measurement with Microbenchmarking} \label{sec:micro-benchmarking}
The performance of a programming language can be difficult to ascertain. Many factors play into the performance of any given block of code, this is especially true in managed languages like Java or C\#. Furthermore, which metrics to measure also play a significant role in the results. Additionally some languages have environments that can increase execution time in certain cases. Microbenchmarks is one of the possibilities within the field of performance benchmarking.

\subsection{Measurement Metrics}
Multiple metrics can be used to measure the performance of code; cycles, \ac{CPU} time or wall clock time. According to \cite{sestoft2013microbenchmarks} wall clock time is the more reliable option. The advantage of wall clock time as a metric is that it is easily comparable to other tests. It furthermore simplifies the testing infrastructure.

\subsection{Language vs. Environment}
Some languages, such as Java and C\#, are managed by a \ac{VM}. When testing performance it is important to account for the resources used by the \ac{VM}. Since certain functionality is outside the control of the programmer it may be difficult to be certain that the garbage collector did not run while the test was executed. In \cite{sestoft2013microbenchmarks} Sestoft, the author of the article, suggests taking the mean running time over a large number of tests are run and computing the standard deviation computed on the resulting time. Furthermore, Sestoft advises not to run the code under test an arbitrary number of times, but rather run the tests multiple times, until the total elapsed time exceeds a quarter second. This gives a much clearer picture of the performance of the language under test.

Using this testing methodology Sestoft achieved some interesting results comparing managed and unmanaged languages \cite{sestoft2010numeric}. Predominately among these is that managed languages where almost as performant as unmanaged languages in many cases and performed better in some.

\subsection{Benchmark Scaffolding} \label{sec:sestoft-scaffolding}
Sestoft's  work with testing C\# and Java is based on some scaffolding, which allows for the automated microbenchmarking of a method. The method must take an integer as a parameter and return a double. Whether these values are used is less important. The parameters and return values are present to prevent the compiler from inlining the function or optimising it away entirely.

Furthermore, the scaffolding takes a duration and a number of iterations as parameters. The duration is the minimum amount of time that the microbenchmark must execute. The mean execution time of the microbenchmark calls is then calculated along side a standard deviation. The iteration parameter determines the amount of times the overall benchmark is run. Therefore, if the minimum execution time is 0.25 seconds and the iterations is set to 5, then the benchmark will run for a total of 1.25 seconds.

The scaffoling implemented by Sestoft can be seen in \lstref{sestoft}
\begin{lstlisting}[language=Java, style=java-highlight, caption={Sestoft's Scaffolding \cite{sestoft2013microbenchmarks}}, label=lst:sestoft]
public static double Mark8  (String msg, String info, IntToD oubleFunction f,
                            int n, double minTime) {
    int count = 1, totalCount = 0;
    double dummy = 0.0, runningTime = 0.0, st = 0.0, sst = 0.0;
    do {
        count*= 2;
        st = sst = 0.0;
        for (int j=0; j<n; j++) {
            Timer t = new Timer();
            for (int i=0; i<count; i++)
                dummy += f.applyAsDouble(i);
            runningTime = t.check();
            double time = runningTime*1e9 / count;
            st += time;
            sst += time*time;
            totalCount += count;
        }
    } while (runningTime < minTime && count < Integer.MAX_VALUE/2);
    double mean = st/n, sdev = Math.sqrt((sst - mean*mean*n)/(n-1));
    System.out.printf("%-25s %s%15.1f %10.2f %10d%n", msg, info, mean, sdev, count);
    return dummy / totalCount;
}
\end{lstlisting}

\tmc{I think we should put a section here, where we summarise our findings.}